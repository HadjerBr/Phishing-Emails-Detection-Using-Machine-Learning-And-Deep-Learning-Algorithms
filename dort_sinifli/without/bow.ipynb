{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4000 normal, 4000 fraudulent, 4000 Suspicious, 4000 Harrasment\n",
    "\n",
    "# CSV dosyasını pandas DataFrame'e yükle\n",
    "df = pd.read_csv('../data/SEFACED.csv')\n",
    "\n",
    "\n",
    "# 'Normal' etiketine sahip satırları filtrele ve ilk 4000 satırı tut\n",
    "normal_rows = df[df['Class_Label'] == 'Normal'].head(4000)\n",
    "\n",
    "# 'Fraudulent' etiketine sahip satırları filtrele ve ilk satırları tut\n",
    "fraudulent_rows = df[df['Class_Label'] == 'Fraudulent'].head(4000)\n",
    "\n",
    "suspicious_rows = df[df['Class_Label'] == 'Suspicious'].head(4000)\n",
    "\n",
    "harrasment_rows = df[df['Class_Label'] == 'Harrasment'].head(4000)\n",
    "\n",
    "# Filtrelenmiş DataFrame'leri birleştir\n",
    "filtered_df = pd.concat([normal_rows, fraudulent_rows, suspicious_rows, harrasment_rows])\n",
    "\n",
    "\n",
    "# Filtrelenmiş DataFrame'i yeni bir CSV dosyasına kaydet\n",
    "filtered_df.to_csv('../data/SEFACED_4000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../data/SEFACED_4000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text Class_Label\n",
      "0  Let me know when you get the quotes from Pauli...      Normal\n",
      "1  ---------------------- Forwarded by Phillip K ...      Normal\n",
      "2  Steve, Please remove Bob Shiring and Liz River...      Normal\n",
      "3  Go ahead and order the ac for #27. Can you ema...      Normal\n",
      "4  Anymore details? Is the offer above or below 6...      Normal\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16000 entries, 0 to 15999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Text         15944 non-null  object\n",
      " 1   Class_Label  16000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 250.1+ KB\n",
      "None\n",
      "                                                     Text Class_Label\n",
      "count                                               15944       16000\n",
      "unique                                              12207           4\n",
      "top     RT @jaboukie: damn a domestic terrorist kills ...      Normal\n",
      "freq                                                  133        4000\n"
     ]
    }
   ],
   "source": [
    "print(df1.head())\n",
    "\n",
    "print(df1.info())\n",
    "\n",
    "print(df1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def load_stopwords(stopwords_file):\n",
    "    with open(stopwords_file, 'r') as file:\n",
    "        stopwords = file.read().splitlines()\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def clean_text(text, stopwords):\n",
    "    if isinstance(text, str):\n",
    "        # Küçük harfe çevir\n",
    "        lowercase_text = text.lower()\n",
    "\n",
    "        # Bağlantıları kaldır\n",
    "        text_no_links = re.sub(r'http\\S+|www.\\S+|\\.com|\\w+@\\w+\\.\\w+', '', lowercase_text)\n",
    "\n",
    "        # HTML etiketlerini kaldır\n",
    "        clean_html = re.sub(r'<.*?>', '', text_no_links)\n",
    "\n",
    "        # Rakamları kaldır\n",
    "        text_no_digits = re.sub(r'\\d+', '', clean_html)\n",
    "\n",
    "        # Özel karakterleri kaldır\n",
    "        text_no_special_chars = re.sub(r'[^a-zA-Z0-9\\s]', '', text_no_digits)\n",
    "\n",
    "        # Metni tokenlara ayır\n",
    "        tokens = text_no_special_chars.split()\n",
    "\n",
    "        # Stop kelimelerini kaldır\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "        # Tokenları tekrar metne birleştir\n",
    "        clean_text = ' '.join(filtered_tokens)\n",
    "\n",
    "        return clean_text\n",
    "\n",
    "def clean_csv(input_csv, output_csv, stopwords_file):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    stopwords = load_stopwords(stopwords_file)\n",
    "\n",
    "    # 'Text' sütununu temizle\n",
    "    df['cleaned_text'] = df['Text'].apply(lambda x: clean_text(x, stopwords))\n",
    "    df = df.drop('Text', axis=1)\n",
    "\n",
    "    # Temizlenmiş veriyi yeni bir CSV dosyasına kaydet\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "input_csv_file = '../data/SEFACED_4000.csv'\n",
    "output_csv_file = '../data/preprocessed_SEFACED.csv'\n",
    "stopwords_file = '../data/stopwords.txt'\n",
    "\n",
    "clean_csv(input_csv_file, output_csv_file, stopwords_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\orange\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Metnin bir dize olup olmadığını kontrol et veya dizeye dönüştür\n",
    "    if isinstance(text, str):\n",
    "        tokens = word_tokenize(text)\n",
    "        return tokens\n",
    "    else:\n",
    "        # Girişin bir dize olmadığı durumda boş bir liste döndür\n",
    "        return []\n",
    "\n",
    "def tokenize_csv(input_csv, output_csv):\n",
    "    # CSV dosyasını oku\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # 'cleaned_text' sütununu tokenle\n",
    "    df['tokens'] = df['cleaned_text'].apply(tokenize_text)\n",
    "    df = df.drop('cleaned_text', axis=1)\n",
    "\n",
    "    # Tokenleştirilmiş veriyi yeni bir CSV dosyasına kaydet\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "input_csv_file = '../data/preprocessed_SEFACED.csv'  \n",
    "output_csv_file = '../data/tokenized_SEFACED.csv' \n",
    "\n",
    "tokenize_csv(input_csv_file, output_csv_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\orange\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\orange\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import ast\n",
    "\n",
    "# İngilizce kelime dağarcığını yükle\n",
    "english_words = set(words.words())\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def remove_misspelled(tokens):\n",
    "    return [token for token in tokens if token in english_words]\n",
    "\n",
    "def lemmatize_csv(input_csv, output_csv):\n",
    "    # CSV dosyasını oku\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # 'tokens' sütununu dizeden liste haline dönüştür\n",
    "    df['tokens'] = df['tokens'].apply(ast.literal_eval)\n",
    "\n",
    "    # 'tokens' sütununu lemmatize et\n",
    "    df['lemmatized_tokens'] = df['tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "    # Yanlış yazılmış kelimeleri kaldır\n",
    "    df['lemmatized_tokens'] = df['lemmatized_tokens'].apply(remove_misspelled)\n",
    "\n",
    "    # Orijinal 'tokens' sütununu kaldır\n",
    "    df = df.drop('tokens', axis=1)\n",
    "\n",
    "    # Lemmatize edilmiş veriyi yeni bir CSV dosyasına kaydet\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Kullanım örneği\n",
    "input_csv_file = '../data/tokenized_SEFACED.csv'\n",
    "output_csv_file = '../data/lemmatized_and_misspelled_removed_SEFACED.csv'\n",
    "lemmatize_csv(input_csv_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       aa  aba  abandon  abandoned  abased  abbas  abbreviation  abdal  \\\n",
      "0       0    0        0          0       0      0             0      0   \n",
      "1       0    0        0          0       0      0             0      0   \n",
      "2       0    0        0          0       0      0             0      0   \n",
      "3       0    0        0          0       0      0             0      0   \n",
      "4       0    0        0          0       0      0             0      0   \n",
      "...    ..  ...      ...        ...     ...    ...           ...    ...   \n",
      "15995   0    0        0          0       0      0             0      0   \n",
      "15996   0    0        0          0       0      0             0      0   \n",
      "15997   0    0        0          0       0      0             0      0   \n",
      "15998   0    0        0          0       0      0             0      0   \n",
      "15999   0    0        0          0       0      0             0      0   \n",
      "\n",
      "       abduction  abey  ...  zing  zip  zipper  zo  zombie  zone  zoned  \\\n",
      "0              0     0  ...     0    0       0   0       0     0      0   \n",
      "1              0     0  ...     0    0       0   0       0     0      0   \n",
      "2              0     0  ...     0    0       0   0       0     0      0   \n",
      "3              0     0  ...     0    0       0   0       0     0      0   \n",
      "4              0     0  ...     0    0       0   0       0     0      0   \n",
      "...          ...   ...  ...   ...  ...     ...  ..     ...   ...    ...   \n",
      "15995          0     0  ...     0    0       0   0       0     0      0   \n",
      "15996          0     0  ...     0    0       0   0       0     0      0   \n",
      "15997          0     0  ...     0    0       0   0       0     0      0   \n",
      "15998          0     0  ...     0    0       0   0       0     0      0   \n",
      "15999          0     0  ...     0    0       0   0       0     0      0   \n",
      "\n",
      "       zoning  zoo  zoom  \n",
      "0           0    0     0  \n",
      "1           0    0     0  \n",
      "2           0    0     0  \n",
      "3           0    0     0  \n",
      "4           0    0     0  \n",
      "...       ...  ...   ...  \n",
      "15995       0    0     0  \n",
      "15996       0    0     0  \n",
      "15997       0    0     0  \n",
      "15998       0    0     0  \n",
      "15999       0    0     0  \n",
      "\n",
      "[16000 rows x 9630 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Ön işlenmiş veriyi oku\n",
    "df = pd.read_csv('../data/lemmatized_and_misspelled_removed_SEFACED.csv', encoding='utf-8')\n",
    "\n",
    "# Lemmatize edilmiş tokenları ayıkla\n",
    "corpus = df['lemmatized_tokens']\n",
    "\n",
    "# Bir CountVectorizer oluştur\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Corpus'u uyum sağla ve dönüştür, seyrek matrise\n",
    "X_bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Seyrek matrisi bir DataFrame'e dönüştür\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(df_bow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.874375\n",
      "Precision: 0.8763977656097423\n",
      "Recall: 0.874375\n",
      "Sınıflandırma Raporu:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Fraudulent       0.96      0.92      0.94       837\n",
      "  Harrasment       0.83      0.84      0.83       737\n",
      "      Normal       0.93      0.94      0.94       767\n",
      "  Suspicious       0.78      0.81      0.79       859\n",
      "\n",
      "    accuracy                           0.87      3200\n",
      "   macro avg       0.88      0.88      0.88      3200\n",
      "weighted avg       0.88      0.87      0.88      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "y = df['Class_Label']\n",
    "\n",
    "# Veri setini eğitim ve test kümelerine ayır\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_bow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Lojistik regresyon modelini eğit\n",
    "model = LogisticRegression(max_iter=10000)  \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test kümesi üzerinde tahminlerde bulun\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Modeli değerlendir\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions, average='weighted')\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "print(\"Sınıflandırma Raporu:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier:\n",
      "Accuracy: 0.8671875\n",
      "Precision: 0.868630001107254\n",
      "Recall: 0.8671875\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Fraudulent       0.87      0.97      0.92       837\n",
      "  Harrasment       0.79      0.85      0.82       737\n",
      "      Normal       0.95      0.92      0.94       767\n",
      "  Suspicious       0.86      0.73      0.79       859\n",
      "\n",
      "    accuracy                           0.87      3200\n",
      "   macro avg       0.87      0.87      0.87      3200\n",
      "weighted avg       0.87      0.87      0.87      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RF\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "# Rastgele Orman modelini başlat\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Rastgele Orman modelini eğit\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Test setinde tahminlerde bulun\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Modeli değerlendir\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "rf_precision = precision_score(y_test, rf_predictions, average='weighted')\n",
    "rf_recall = recall_score(y_test, rf_predictions, average='weighted')\n",
    "\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier:\n",
      "Accuracy: 0.84375\n",
      "Precision: 0.8699893846411478\n",
      "Recall: 0.84375\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Fraudulent       0.97      0.89      0.93       837\n",
      "  Harrasment       0.66      0.94      0.78       737\n",
      "      Normal       0.90      0.93      0.91       767\n",
      "  Suspicious       0.93      0.64      0.76       859\n",
      "\n",
      "    accuracy                           0.84      3200\n",
      "   macro avg       0.86      0.85      0.84      3200\n",
      "weighted avg       0.87      0.84      0.84      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NB\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "# Naive Bayes modelini başlat\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Naive Bayes modelini eğit\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Test seti üzerinde tahminler yap\n",
    "nb_predictions = nb_model.predict(X_test)\n",
    "\n",
    "# Modeli değerlendir\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "nb_precision = precision_score(y_test, nb_predictions, average='weighted')\n",
    "nb_recall = recall_score(y_test, nb_predictions, average='weighted')\n",
    "\n",
    "print(\"Naive Bayes Classifier:\")\n",
    "print(f\"Accuracy: {nb_accuracy}\")\n",
    "print(f\"Precision: {nb_precision}\")\n",
    "print(f\"Recall: {nb_recall}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, nb_predictions))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Classifier:\n",
      "Accuracy: 0.850625\n",
      "Precision: 0.8711681084999202\n",
      "Recall: 0.850625\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Fraudulent       0.98      0.86      0.92       837\n",
      "  Harrasment       0.88      0.77      0.82       737\n",
      "      Normal       0.95      0.87      0.91       767\n",
      "  Suspicious       0.68      0.89      0.77       859\n",
      "\n",
      "    accuracy                           0.85      3200\n",
      "   macro avg       0.87      0.85      0.86      3200\n",
      "weighted avg       0.87      0.85      0.85      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM modelini başlat\n",
    "svm_model = SVC()\n",
    "\n",
    "# SVM modelini eğit\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Test seti üzerinde tahminler yap\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "# Modeli değerlendir\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "svm_precision = precision_score(y_test, svm_predictions, average='weighted')\n",
    "svm_recall = recall_score(y_test, svm_predictions, average='weighted')\n",
    "\n",
    "print(\"Support Vector Machine Classifier:\")\n",
    "print(f\"Accuracy: {svm_accuracy}\")\n",
    "print(f\"Precision: {svm_precision}\")\n",
    "print(f\"Recall: {svm_recall}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, svm_predictions))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a06e30aee820ff8fe24fb428cece021c5cdfc4bd84062afc0eba853e87fef366"
  },
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
