{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel dosyasını pandas DataFrame'e yükle\n",
    "df = pd.read_excel('./data_without_extra_features/SEFACED.xlsx')\n",
    "\n",
    "# DataFrame'i bir CSV dosyasına kaydet\n",
    "df.to_csv('./data_without_extra_features/SEFACED.csv', index=False)  # İndeks sütunu olarak satır numaralarının kaydedilmesini önlemek için index=False olarak ayarlayın\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9000 normal, 9000 fraudulent\n",
    "\n",
    "# CSV dosyasını pandas DataFrame'e yükle\n",
    "df = pd.read_csv('./data_without_extra_features/SEFACED.csv')\n",
    "\n",
    "\n",
    "# 'Normal' etiketine sahip satırları filtrele ve ilk 4000 satırı tut\n",
    "normal_rows = df[df['Class_Label'] == 'Normal']\n",
    "\n",
    "\n",
    "# 'Fraudulent' etiketine sahip satırları filtrele ve ilk satırları tut\n",
    "fraudulent_rows = df[df['Class_Label'] == 'Fraudulent']\n",
    "\n",
    "# Filtrelenmiş DataFrame'leri birleştir\n",
    "filtered_df = pd.concat([normal_rows, fraudulent_rows])\n",
    "\n",
    "\n",
    "# Filtrelenmiş DataFrame'i yeni bir CSV dosyasına kaydet\n",
    "filtered_df.to_csv('./data_without_extra_features/SEFACED_9000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./data_without_extra_features/SEFACED_9000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text Class_Label\n",
      "0  Let me know when you get the quotes from Pauli...      Normal\n",
      "1  ---------------------- Forwarded by Phillip K ...      Normal\n",
      "2  Steve, Please remove Bob Shiring and Liz River...      Normal\n",
      "3  Go ahead and order the ac for #27. Can you ema...      Normal\n",
      "4  Anymore details? Is the offer above or below 6...      Normal\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17640 entries, 0 to 17639\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Text         17541 non-null  object\n",
      " 1   Class_Label  17640 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 275.8+ KB\n",
      "None\n",
      "                                                    Text Class_Label\n",
      "count                                              17541       17640\n",
      "unique                                             10670           2\n",
      "top     <html><head><style>P{margin:0px;padding:0px}body      Normal\n",
      "freq                                                  49       12498\n"
     ]
    }
   ],
   "source": [
    "print(df1.head())\n",
    "\n",
    "print(df1.info())\n",
    "\n",
    "print(df1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def load_stopwords(stopwords_file):\n",
    "    with open(stopwords_file, 'r') as file:\n",
    "        stopwords = file.read().splitlines()\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def clean_text(text, stopwords):\n",
    "    if isinstance(text, str):\n",
    "        # Küçük harfe çevir\n",
    "        lowercase_text = text.lower()\n",
    "\n",
    "        # Bağlantıları kaldır\n",
    "        text_no_links = re.sub(r'http\\S+|www.\\S+|\\.com|\\w+@\\w+\\.\\w+', '', lowercase_text)\n",
    "\n",
    "        # HTML etiketlerini kaldır\n",
    "        clean_html = re.sub(r'<.*?>', '', text_no_links)\n",
    "\n",
    "        # Rakamları kaldır\n",
    "        text_no_digits = re.sub(r'\\d+', '', clean_html)\n",
    "\n",
    "        # Özel karakterleri kaldır\n",
    "        text_no_special_chars = re.sub(r'[^a-zA-Z0-9\\s]', '', text_no_digits)\n",
    "\n",
    "        # Metni tokenlara ayır\n",
    "        tokens = text_no_special_chars.split()\n",
    "\n",
    "        # Stop kelimelerini kaldır\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "        # Tokenları tekrar metne birleştir\n",
    "        clean_text = ' '.join(filtered_tokens)\n",
    "\n",
    "        return clean_text\n",
    "\n",
    "def clean_csv(input_csv, output_csv, stopwords_file):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    stopwords = load_stopwords(stopwords_file)\n",
    "\n",
    "    # 'Text' sütununu temizle\n",
    "    df['cleaned_text'] = df['Text'].apply(lambda x: clean_text(x, stopwords))\n",
    "    df = df.drop('Text', axis=1)\n",
    "\n",
    "    # Temizlenmiş veriyi yeni bir CSV dosyasına kaydet\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "input_csv_file = './data_without_extra_features/SEFACED_9000.csv'\n",
    "output_csv_file = './data_without_extra_features/preprocessed_SEFACED.csv' \n",
    "stopwords_file = './stopwords.txt'\n",
    "\n",
    "clean_csv(input_csv_file, output_csv_file, stopwords_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\orange\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Metnin bir dize olup olmadığını kontrol et veya dizeye dönüştür\n",
    "    if isinstance(text, str):\n",
    "        tokens = word_tokenize(text)\n",
    "        return tokens\n",
    "    else:\n",
    "        # Girişin bir dize olmadığı durumda boş bir liste döndür\n",
    "        return []\n",
    "\n",
    "def tokenize_csv(input_csv, output_csv):\n",
    "    # CSV dosyasını oku\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # 'cleaned_text' sütununu tokenle\n",
    "    df['tokens'] = df['cleaned_text'].apply(tokenize_text)\n",
    "    df = df.drop('cleaned_text', axis=1)\n",
    "\n",
    "    # Tokenleştirilmiş veriyi yeni bir CSV dosyasına kaydet\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "input_csv_file = './data_without_extra_features/preprocessed_SEFACED.csv'  \n",
    "output_csv_file = './data_without_extra_features/tokenized_SEFACED.csv' \n",
    "\n",
    "tokenize_csv(input_csv_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\orange\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\orange\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import ast\n",
    "\n",
    "# İngilizce kelime dağarcığını yükle\n",
    "english_words = set(words.words())\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def remove_misspelled(tokens):\n",
    "    return [token for token in tokens if token in english_words]\n",
    "\n",
    "def lemmatize_csv(input_csv, output_csv):\n",
    "    # CSV dosyasını oku\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # 'tokens' sütununu dizeden liste haline dönüştür\n",
    "    df['tokens'] = df['tokens'].apply(ast.literal_eval)\n",
    "\n",
    "    # 'tokens' sütununu lemmatize et\n",
    "    df['lemmatized_tokens'] = df['tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "    # Yanlış yazılmış kelimeleri kaldır\n",
    "    df['lemmatized_tokens'] = df['lemmatized_tokens'].apply(remove_misspelled)\n",
    "\n",
    "    # Orijinal 'tokens' sütununu kaldır\n",
    "    df = df.drop('tokens', axis=1)\n",
    "\n",
    "    # Lemmatize edilmiş veriyi yeni bir CSV dosyasına kaydet\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Kullanım örneği\n",
    "input_csv_file = './data_without_extra_features/tokenized_SEFACED.csv'\n",
    "output_csv_file = './data_without_extra_features/lemmatized_and_misspelled_removed_SEFACED.csv'\n",
    "lemmatize_csv(input_csv_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       aa  aba  abandon  abandoned  abandonment  abased  abatement  abb  \\\n",
      "0       0    0        0          0            0       0          0    0   \n",
      "1       0    0        0          0            0       0          0    0   \n",
      "2       0    0        0          0            0       0          0    0   \n",
      "3       0    0        0          0            0       0          0    0   \n",
      "4       0    0        0          0            0       0          0    0   \n",
      "...    ..  ...      ...        ...          ...     ...        ...  ...   \n",
      "17635   0    0        0          0            0       0          0    0   \n",
      "17636   0    0        0          0            0       0          0    0   \n",
      "17637   0    0        0          0            0       0          0    0   \n",
      "17638   0    0        0          0            0       0          0    0   \n",
      "17639   0    0        0          0            0       0          0    0   \n",
      "\n",
      "       abbas  abbey  ...  zinc  zinfandel  zing  zip  zipper  zonal  zone  \\\n",
      "0          0      0  ...     0          0     0    0       0      0     0   \n",
      "1          0      0  ...     0          0     0    0       0      0     0   \n",
      "2          0      0  ...     0          0     0    0       0      0     0   \n",
      "3          0      0  ...     0          0     0    0       0      0     0   \n",
      "4          0      0  ...     0          0     0    0       0      0     0   \n",
      "...      ...    ...  ...   ...        ...   ...  ...     ...    ...   ...   \n",
      "17635      0      0  ...     0          0     0    0       0      0     0   \n",
      "17636      0      0  ...     0          0     0    0       0      0     0   \n",
      "17637      0      0  ...     0          0     0    0       0      0     0   \n",
      "17638      0      0  ...     0          0     0    0       0      0     0   \n",
      "17639      0      0  ...     0          0     0    0       0      0     0   \n",
      "\n",
      "       zoning  zoo  zoom  \n",
      "0           0    0     0  \n",
      "1           0    0     0  \n",
      "2           0    0     0  \n",
      "3           0    0     0  \n",
      "4           0    0     0  \n",
      "...       ...  ...   ...  \n",
      "17635       0    0     0  \n",
      "17636       0    0     0  \n",
      "17637       0    0     0  \n",
      "17638       0    0     0  \n",
      "17639       0    0     0  \n",
      "\n",
      "[17640 rows x 12851 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Ön işlenmiş veriyi oku\n",
    "df = pd.read_csv('./data_without_extra_features/lemmatized_and_misspelled_removed_SEFACED.csv', encoding='utf-8')\n",
    "\n",
    "# Lemmatize edilmiş tokenları ayıkla\n",
    "corpus = df['lemmatized_tokens']\n",
    "\n",
    "# Bir CountVectorizer oluştur\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Corpus'u uyum sağla ve dönüştür, seyrek matrise\n",
    "X_bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Seyrek matrisi bir DataFrame'e dönüştür\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(df_bow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "import joblib\n",
    "\n",
    "\n",
    "y = df['Class_Label']\n",
    "\n",
    "# Veri setini eğitim ve test kümelerine ayır\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_bow, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9705215419501134\n",
      "Precision: 0.9708492507431445\n",
      "Recall: 0.9705215419501134\n",
      "Sınıflandırma Raporu:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Fraudulent       0.98      0.92      0.95      1041\n",
      "      Normal       0.97      0.99      0.98      2487\n",
      "\n",
      "    accuracy                           0.97      3528\n",
      "   macro avg       0.97      0.95      0.96      3528\n",
      "weighted avg       0.97      0.97      0.97      3528\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/lr_vectorizer.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "import joblib\n",
    "\n",
    "\n",
    "y = df['Class_Label']\n",
    "\n",
    "# Veri setini eğitim ve test kümelerine ayır\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_bow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Lojistik regresyon modelini eğit\n",
    "model = LogisticRegression(max_iter=10000)  \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test kümesi üzerinde tahminlerde bulun\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Modeli değerlendir\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions, average='weighted')\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "print(\"Sınıflandırma Raporu:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "\n",
    "joblib.dump(model, './models/bow_lr_trained_model.pkl')\n",
    "joblib.dump(vectorizer, './models/lr_vectorizer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier:\n",
      "Accuracy: 0.9750566893424036\n",
      "Precision: 0.9752482665059014\n",
      "Recall: 0.9750566893424036\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Fraudulent       0.98      0.93      0.96      1041\n",
      "      Normal       0.97      0.99      0.98      2487\n",
      "\n",
      "    accuracy                           0.98      3528\n",
      "   macro avg       0.98      0.96      0.97      3528\n",
      "weighted avg       0.98      0.98      0.97      3528\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/bow_rf_trained_model.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RF\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "# Rastgele Orman modelini başlat\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Rastgele Orman modelini eğit\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Test setinde tahminlerde bulun\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Modeli değerlendir\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "rf_precision = precision_score(y_test, rf_predictions, average='weighted')\n",
    "rf_recall = recall_score(y_test, rf_predictions, average='weighted')\n",
    "\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "\n",
    "# Eğitilmiş modeli kaydet\n",
    "joblib.dump(rf_model, './models/bow_rf_trained_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier:\n",
      "Accuracy: 0.9623015873015873\n",
      "Precision: 0.9628056385597717\n",
      "Recall: 0.9623015873015873\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Fraudulent       0.98      0.89      0.93      1041\n",
      "      Normal       0.96      0.99      0.97      2487\n",
      "\n",
      "    accuracy                           0.96      3528\n",
      "   macro avg       0.97      0.94      0.95      3528\n",
      "weighted avg       0.96      0.96      0.96      3528\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/bow_nb_trained_model.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "# Naive Bayes modelini başlat\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Naive Bayes modelini eğit\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Test seti üzerinde tahminler yap\n",
    "nb_predictions = nb_model.predict(X_test)\n",
    "\n",
    "# Modeli değerlendir\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "nb_precision = precision_score(y_test, nb_predictions, average='weighted')\n",
    "nb_recall = recall_score(y_test, nb_predictions, average='weighted')\n",
    "\n",
    "print(\"Naive Bayes Classifier:\")\n",
    "print(f\"Accuracy: {nb_accuracy}\")\n",
    "print(f\"Precision: {nb_precision}\")\n",
    "print(f\"Recall: {nb_recall}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, nb_predictions))\n",
    "\n",
    "# Eğitilmiş modeli kaydet\n",
    "joblib.dump(nb_model, './models/bow_nb_trained_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM modelini başlat\n",
    "svm_model = SVC()\n",
    "\n",
    "# SVM modelini eğit\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Test seti üzerinde tahminler yap\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "# Modeli değerlendir\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "svm_precision = precision_score(y_test, svm_predictions, average='weighted')\n",
    "svm_recall = recall_score(y_test, svm_predictions, average='weighted')\n",
    "\n",
    "print(\"Support Vector Machine Classifier:\")\n",
    "print(f\"Accuracy: {svm_accuracy}\")\n",
    "print(f\"Precision: {svm_precision}\")\n",
    "print(f\"Recall: {svm_recall}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, svm_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
